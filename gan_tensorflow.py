# -*- coding: utf-8 -*-
"""gan_tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t3Oj4NGswji4zc4Vph4mGnDVTuCa4hIF
"""

'''
================Part1. Understanding GAN models basic concepts==================
Reference:
1. What are two basic part in generative model?
      Ans: The generator and the discriminator.
2. What is the specific objective of these two part?
      Ans: <G>: Generate fake-samples to confuse the distinction of <D>.
           <D>: A binary classifier here. Try it best to distinguish the fake and real.
3. What is the basic loss function of GAN.
      Ans: Cross Entropy by minmax(V(G,D)). V is accuracy. log(1-D(G())
4. What is training	process of basic GAN model?
      Ans: For each steps of iterations.
        Firstly training the discriminator (with BP of <G> is stopped);
        Then training the Generator (with BP of <D> is stopped).

Comprehension of Code
Ques1.
      Dataset: MNIST dataset(LeCun) by using 'from tensorflow.examples.tutorials.mnist import input_data'
      Model:   The image of D&G. Random Noise been put into G and output the G_img; then put G_img and real_img into D, output 0 or 1
      Loss:    Cross Entropy, here the D is: tf.reduce_mean(-tf.log(D_real + eps) - tf.log(1 - D_fake + eps))
                                       G is: tf.reduce_mean(-tf.log(D_fake + eps))
      Training:epoch=100,D first, then G(for each stop BP during training)
      Test:    show_result((epoch + 1), save=True, path=p),save model
Ques2.
hint:Adam for gradient descent




================Part2. GANs with TF==================
Ques before task:
1. What is dropout in deep learning and its advantages.
      Ans: randomly make some units in each layers become ‘0’ with a probability
           avoid overfitting and spend less time
2. List some typical optimizers in deep learning.
      Ans: SGD, BGD, Adam, Momentum, RMSprop, minibatch
3. What optimizer we used for training in this case.
      Ans: Adam
4. How do D-loss and G-loss change during training? Visualize how the D-loss and D-loss change during training and explain why.
      Ans: as the pic shown in the report or google doc

'''

import os, time, itertools, pickle
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

#hint: reset the graph of TF
tf.reset_default_graph()

# G(z)
#============================Original G(z)======================================
def generator_ori(x):
    # initializers
    w_init = tf.truncated_normal_initializer(mean=0, stddev=0.02)#generate the Gaussian random. weight could not be 0
    b_init = tf.constant_initializer(0.)#bias could be 0
    # 1st hidden layer
    #Generate G_Weight_layer1, the dimension is [x.列数, 128]
    w0 = tf.get_variable('G_w0', [x.get_shape()[1], 128], initializer=w_init)
    b0 = tf.get_variable('G_b0', [128], initializer=b_init)#Generate G_bias_layer1, the dimension is [128,1]
    h0 = tf.nn.relu(tf.matmul(x, w0) + b0)#layer1 relu activation func
    # output hidden layer
    w1 = tf.get_variable('G_w1', [h0.get_shape()[1], 784], initializer=w_init)#[layer1_output's col, 784],784=28*28
    b1 = tf.get_variable('G_b1', [784], initializer=b_init)#[784,1]
    o = tf.nn.tanh(tf.matmul(h0, w1) + b1)#output tanh activation func (-1~1)
    return o




#============================ToDO code1. new G(z)===============================
### Code:ToDO( Change the architecture as CW2 Guidance required)
def generator(x):
    # initializers
    w_init = tf.truncated_normal_initializer(mean=0, stddev=0.02)#Gaussian
    b_init = tf.constant_initializer(0.)#bias=0
    # 1st hidden layer
    w0 = tf.get_variable('G_w0', [x.get_shape()[1], 256], initializer=w_init)#[x's col, 256]
    b0 = tf.get_variable('G_b0', [256], initializer=b_init)#[256,1]
    h0 = tf.nn.leaky_relu(tf.matmul(x, w0) + b0)#layer1 leaky_relu activation
    # 2nd hidden layer
    w1 = tf.get_variable('G_w1', [h0.get_shape()[1], 512], initializer=w_init)#[h0's col, 512]
    b1 = tf.get_variable('G_b1', [512], initializer=b_init)#[512,1]
    h1 = tf.nn.leaky_relu(tf.matmul(h0, w1) + b1)#layer2 leaky_relu activation
    # 3rd hidden layer
    w2 = tf.get_variable('G_w2', [h1.get_shape()[1], 1024], initializer=w_init)#[h1's cols, 1024]
    b2 = tf.get_variable('G_b2', [1024], initializer=b_init)#[1024,1]
    h2 = tf.nn.leaky_relu(tf.matmul(h1, w2) + b2)#layer3 leaky_relu activation
    # output hidden layer
    w_out = tf.get_variable('G_wout', [h2.get_shape()[1], 784], initializer=w_init)#[layer1_output's cols, 784],784=28*28
    b_out = tf.get_variable('G_bout', [784], initializer=b_init)#[784,1]
    o = tf.nn.tanh(tf.matmul(h2, w_out) + b_out)#output tanh activation (-1~1)
    return o

# D(x)
#============================Original D(x)======================================
def discriminator_ori(x, drop_out):
    # initializers
    w_init = tf.truncated_normal_initializer(mean=0, stddev=0.02)#Gaussian
    b_init = tf.constant_initializer(0.)#bias=0
    # 1st hidden layer
    w0 = tf.get_variable('D_w0', [x.get_shape()[1], 784], initializer=w_init)#cuz the input of MNIST is the img=28x28,reshape as [784].
                                                                             #Here the dimension is [x's cols, 784]. Here x is [550,784],col=784
    b0 = tf.get_variable('D_b0', [784], initializer=b_init)#bias layer, [784,1]
    h0 = tf.nn.relu(tf.matmul(x, w0) + b0)#layer1 relu activation
    # output layer
    w1 = tf.get_variable('D_w1', [h0.get_shape()[1], 1], initializer=w_init)#[layer1_output's cols, 1]
    b1 = tf.get_variable('D_b1', [1], initializer=b_init)#[1,1]
    o = tf.sigmoid(tf.matmul(h0, w1) + b1)#output sigmoid activation (0~1), distinguish fake(0) or real(1)

    return o




#=============================ToDO code1. new D(x)==============================
###  Code: ToDO( Change the architecture as CW2 Guidance required)
def discriminator(x, drop_out):#drop_out=0.3 by using placeholder in line 238 & 298
    # initializers
    w_init = tf.truncated_normal_initializer(mean=0, stddev=0.02)#Gaussian
    b_init = tf.constant_initializer(0.)#bias=0
    # 1st hidden layer
    w0 = tf.get_variable('D_w0', [x.get_shape()[1], 1024], initializer=w_init)#cuz the input of MNIST is the img=28x28,reshape as [784].
                                                                             #Here the dimension is [x's cols, 1024],here x's cols = 784
    b0 = tf.get_variable('D_b0', [1024], initializer=b_init)#bias layer, [1024,1]
    h0_ = tf.nn.leaky_relu(tf.matmul(x, w0) + b0)#layer1 leaky_relu activation
      #drop out
    h0 = tf.nn.dropout(h0_, keep_prob=drop_out)
    # 2nd hidden layer
    w1 = tf.get_variable('D_w1', [h0.get_shape()[1], 512], initializer=w_init)
    b1 = tf.get_variable('D_b1', [512], initializer=b_init)
    h1_ = tf.nn.leaky_relu(tf.matmul(h0, w1) + b1)#layer2 leaky_relu activation
       #drop out
    h1 = tf.nn.dropout(h1_, keep_prob=drop_out)
    # 3rd hidden layer
    w2 = tf.get_variable('D_w2', [h1.get_shape()[1], 256], initializer=w_init)
    b2 = tf.get_variable('D_b2', [256], initializer=b_init)
    h2_ = tf.nn.leaky_relu(tf.matmul(h1, w2) + b2)#layer3 leaky_relu activation
       #drop out
    h2 = tf.nn.dropout(h2_, keep_prob=drop_out)
    # output layer
    w_out = tf.get_variable('D_wout', [h2.get_shape()[1], 1], initializer=w_init)#[layer1_output's col, 1]
    b_out = tf.get_variable('D_bout', [1], initializer=b_init)#[1,1]
    o = tf.sigmoid(tf.matmul(h2, w_out) + b_out)#output sigmoid activation (0~1),real(1) or fake(0)

    return o

def show_result(num_epoch, show = False, save = False, path = 'result.png'):
    z_ = np.random.normal(0, 1, (25, 100))    # z_ is the input of generator, every epochs will random produce input
    ##Code:ToDo complete the rest of part

    test_images = sess.run(G_z, {z: z_})#the output's dimension is 25x784 that has been used tanh to deal
                                        #which means 25 images, each the shape is 28x28=784
    fig, ax = plt.subplots(5, 5)
    #use itertolls to hide the axis
    for i, j in itertools.product(range(5), range(5)):
        ax[i, j].get_xaxis().set_visible(False)
        ax[i, j].get_yaxis().set_visible(False)
    for k in range(5*5):
        i = k // 5
        j = k % 5
        ax[i, j].cla()#clean the axis area
        ax[i, j].imshow(np.reshape(test_images[k], (28, 28)), cmap='gray')#reshape for showing
    label = 'Step {0}'.format(num_epoch)
    fig.text(0.5, 0.04, label, ha='center')
    if save:
        plt.savefig(path)
    if show:
        plt.show()
    else:
        plt.close()

'''
creat a dictionary 'train_hist' to save: D_losses, G_losses, per_epoch_ptimes, total_ptime
<print the result like this>:
    [1/100] - ptime: 20.01 loss_d: 0.510, loss_g: 1.048
    *here the 'ptime', 'loss_d' and 'loss_g' are the data in dictionary
<change the code here to show the result>:
    show_train_hist(train_hist, save=True, path='MNIST_GAN_results/MNIST_GAN_train_hist.png')

'''
def show_train_hist(hist, show = True, save = False, path = 'Train_hist.png'):
    x = range(len(hist['D_losses']))

    y1 = hist['D_losses']
    y2 = hist['G_losses']

    plt.plot(x, y1, label='D_loss')
    plt.plot(x, y2, label='G_loss')

    plt.xlabel('Epoch')
    plt.ylabel('Loss')

    plt.legend(loc=4)
    plt.grid(True)
    plt.tight_layout()

    if save:
        plt.savefig(path)

    if show:
        plt.show()
    else:
        plt.close()

# training parameters
#Ques1.3 change batch_size to 256
batch_size = 100
#batch_size = 256
lr = 0.0002
#Ques1.2 change learning rate to 0.01
#lr = 0.01
#Ques1.3 change epoch to 200
train_epoch = 100
#train_epoch = 200

# load MNIST,[55000,784]
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)#one-hot
train_set = (mnist.train.images - 0.5) / 0.5  # normalization; range: -1 ~ 1 by using [pix_val - 0.5]/0.5

# networks : generator
with tf.variable_scope('G'):
    z = tf.placeholder(tf.float32, shape=(None, 100))
    G_z = generator(z)

# networks : discriminator
with tf.variable_scope('D') as scope:
    drop_out = tf.placeholder(dtype=tf.float32, name='drop_out')#avoid overfitting
    x = tf.placeholder(tf.float32, shape=(None, 784))
    D_real = discriminator(x, drop_out)
    scope.reuse_variables()#tell TF that we will re-use these parameter
    D_fake = discriminator(G_z, drop_out)#G_z is to generate each step's data

# loss for each network
#======================important:design the loss_func===============================
#eps is the loss parameter,used to avoid the value of loss equal to 0 or 1 to avoid mistake in GD
eps = 1e-2
#reduce_mean used to calculate the 'mean' of one axis
#here use negative for a stable Gradient Descent Calculating
#hinge_loss
D_loss = tf.reduce_mean(-tf.log(D_real + eps) - tf.log(1 - D_fake + eps))
G_loss = tf.reduce_mean(-tf.log(D_fake + eps))

# trainable variables for each network
#return a list of variables
t_vars = tf.trainable_variables()
#extract D_vars and G_vars from t_vars
D_vars = [var for var in t_vars if 'D_' in var.name]
G_vars = [var for var in t_vars if 'G_' in var.name]

# optimizer for each network
# Define two AdamGD Optimizers to minimize the loss
D_optim = tf.train.AdamOptimizer(lr).minimize(D_loss, var_list=D_vars)
G_optim = tf.train.AdamOptimizer(lr).minimize(G_loss, var_list=G_vars)

# open session and initialize all variables
sess = tf.InteractiveSession()
tf.global_variables_initializer().run()

# results save folder
# judge whether it is a direction
if not os.path.isdir('MNIST_GAN_results'):
# if not we will generate a new one
    os.mkdir('MNIST_GAN_results')
if not os.path.isdir('MNIST_GAN_results/results'):
    os.mkdir('MNIST_GAN_results/results')
train_hist = {}
train_hist['D_losses'] = []
train_hist['G_losses'] = []
train_hist['per_epoch_ptimes'] = []
train_hist['total_ptime'] = []

# training-loop
# random seed to keep the random number in a fixed range, here the range is the time when running
np.random.seed(int(time.time()))
start_time = time.time()
for epoch in range(train_epoch):#0-100
    G_losses = []
    D_losses = []
    epoch_start_time = time.time()
    for iter in range(train_set.shape[0] // batch_size):#55000 pics in 100 batches.
      #train D first without training G, train G then without

        # update discriminator
        x_ = train_set[iter*batch_size:(iter+1)*batch_size]#batch1:0-549
        z_ = np.random.normal(0, 1, (batch_size, 100))#latent vector of Gaussian Noise input,0 is the mean and 1 is standard-value
#keep_prob=1即可不进行dropout
        loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, z: z_, drop_out: 0.3})#use a dictionary for convinent
        D_losses.append(loss_d_)#save each batch to calculate the loss and save into D_loss for calculate the mean

        # update generator
        z_ = np.random.normal(0, 1, (batch_size, 100))
        loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, drop_out: 0.3})#as 5 lines before
        G_losses.append(loss_g_)#as 5 lines before

    epoch_end_time = time.time()
    per_epoch_ptime = epoch_end_time - epoch_start_time#iterated one loop, calculate the time cost
    print('[%d/%d] - ptime: %.2f loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, np.mean(D_losses), np.mean(G_losses)))

    ### Code: TODO Code complet show_result function)
    p = 'MNIST_GAN_results/results/MNIST_GAN_' + str(epoch + 1) + '.png'
    #show when epoch=10，20，50，100
    if (epoch == 9) | (epoch ==19) | (epoch ==49) | (epoch==99):
      show_result((epoch + 1), show = True, save=True, path=p)
    train_hist['D_losses'].append(np.mean(D_losses))
    train_hist['G_losses'].append(np.mean(G_losses))
    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)

end_time = time.time()
total_ptime = end_time - start_time#time cost
train_hist['total_ptime'].append(total_ptime)
print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(train_hist['per_epoch_ptimes']), train_epoch, total_ptime))
print("Training finish!... save training results")

with open('MNIST_GAN_results/train_hist.pkl', 'wb') as f:
    pickle.dump(train_hist, f)
show_train_hist(train_hist, save=True, path='MNIST_GAN_results/MNIST_GAN_train_hist.png')
images = []
sess.close()

sess.close()

'''
<实验记录>
    第一次实验：
        Learning Rate:       lr = 0.0002
        Avg per epoch ptime: 19.81, total 100 epochs ptime: 7896.81
        loss_d变化:

        drop_out:            初次试验并没有用到,在D中定义了,在'with variable_scope' of D中placeholder传入

'''

sess.close()
